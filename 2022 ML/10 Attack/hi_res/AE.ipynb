{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f43b4-e010-4305-a1da-2a949df9eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from typing import List, Dict\n",
    "import copy\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile, os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "supported_models = ['resnet18', 'resnet50', 'resnet101', 'vgg16_bn', 'vgg19_bn', 'inception_v3']\n",
    "model_name = supported_models[2]\n",
    "verify_model_name = supported_models[4]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# if str(device) == 'cpu':\n",
    "#     raise RuntimeError(\"cuda is NOT available!!\")\n",
    "\n",
    "benign_pic = './data/cat/Cat04.jpg'\n",
    "benign_pic = './data/duck/Duck02.jpg'\n",
    "benign_pic = './data/mouse/Mouse06.jpg'\n",
    "\n",
    "\n",
    "target_id = 508 # computer_keyboard\n",
    "target_id = 99 # goose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8026ab-9a78-4306-9db9-75a8ed3fae7a",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653f2cb-6aa4-4bb3-9dbf-2c673a7e1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean=torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "imagenet_std=torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589593d6-7c04-4765-acc8-e2ca63247fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon = 8 / 255. / imagenet_std\n",
    "epsilon = 8 / 255. * imagenet_std\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a7d99-5c86-4b78-b7f5-2314f70a43ee",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09836601-88b7-4e21-9820-253d58dbeac6",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59caec1-9968-4916-8ec7-8610b4ef9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize(preprocess_image(benign_pic), adv_x, predicted_class, benign_confidence, adv_predicted_class, adv_confidence)\n",
    "def visualize(x, adv, benign_label:int, adv_label:int, benign_confidence:float, adv_confidence:float, height:int=10, width:int=20):\n",
    "    def restore(x):\n",
    "        # x = x * imagenet_std + imagenet_mean\n",
    "        return x\n",
    "\n",
    "    x, adv = restore(x.detach().cpu()), restore(adv.detach().cpu())\n",
    "    x, adv = x.numpy().transpose([0, 2, 3, 1]), adv.numpy().transpose([0, 2, 3, 1]) # transpose (bs, C, H, W) back to (bs, H, W, C)\n",
    "    plt.figure(figsize=(height, width))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    # predicted_class, predicted_classname, confidence = classify_image(model, benign_pic, class_labels)\n",
    "    plt.title(f\"Benign: {class_labels[benign_label]} (confidence: {benign_confidence:.1%})\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.squeeze())\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f\"Adversary: {class_labels[adv_label]} (confidence: {adv_confidence:.1%})\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(adv.squeeze())\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f428e-49b0-4bf5-a763-aa504c56024a",
   "metadata": {},
   "source": [
    "## Store & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106bf6e-140f-43de-9b0d-b140501a9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "def store_img_from_tensor(tensor_image, img_path:str):\n",
    "    if not re.match(r'.*\\.png$', img_path, re.IGNORECASE):\n",
    "        raise TypeError(f\"We have to store image file to png due to the `loss nature of JPEG format`!\")\n",
    "\n",
    "    # tensor_image = tensor_image * imagenet_std + imagenet_mean\n",
    "    # Define the transformation to apply to the tensor\n",
    "    transform = transforms.ToPILImage()\n",
    "    # Apply the transformation to the tensor\n",
    "    pil_image = transform(tensor_image.squeeze())\n",
    "    # Save the PIL image to disk\n",
    "    pil_image.save(img_path) # , quality=100) - not necessary as png, unlike jpeg, will not lose quality\n",
    "    \n",
    "def load_img_to_tensor(img_path:str):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Load the image using PIL\n",
    "    pil_image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Define the transformation to apply to the image\n",
    "    img_loader = transforms.Compose([\n",
    "        transforms.ToTensor(), # Convert the image to a tensor\n",
    "        # transforms.Resize(size=8), # 256),\n",
    "        # transforms.CenterCrop(size=6), #224),\n",
    "        # transforms.Normalize(mean=imagenet_mean, std=imagenet_std) # Normalize the image\n",
    "    ])\n",
    "\n",
    "    # Apply the transformation to the PIL image\n",
    "    tensor_image = img_loader(pil_image)\n",
    "    tensor_image = torch.unsqueeze(tensor_image, 0)  # Add a batch dimension\n",
    "\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57361ea-4168-42a4-96ce-bf8709cdeaf0",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d00f9-0402-416c-b959-791c2ec611b4",
   "metadata": {},
   "source": [
    "Model list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py).\n",
    "\n",
    "Model label list is ImageNet labels which can be found [here](https://files.fast.ai/models/imagenet_class_index.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7873e6-2f99-4ca9-8db5-4d21e2e0d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory():\n",
    "    _instance = None\n",
    "    _supported_models = []\n",
    "    _models = {}\n",
    "    _class_labels = None\n",
    "\n",
    "    def _fill_classlabels(self):\n",
    "        class_file = 'imagenet_class_index.json'\n",
    "        with open(class_file, 'r') as f:\n",
    "            f_contents = f.read()\n",
    "        class_labels = json.loads(f_contents)\n",
    "        self._class_labels = {int(k):v[1] for k, v in class_labels.items()}\n",
    "        \n",
    "    def __new__(self, supported_models, *args, **kwargs):\n",
    "        if not self._instance:\n",
    "            self._instance = super(ModelFactory, self).__new__(self, *args, **kwargs)\n",
    "            self._supported_models = copy.deepcopy(supported_models)\n",
    "            self._models = dict()\n",
    "            self._fill_classlabels(self)\n",
    "        return self._instance\n",
    "            \n",
    "    def get_supported_models(self)->List[str]:\n",
    "        return self.supported_models\n",
    "\n",
    "    def get_model(self, model_name):\n",
    "        m = model_name.lower()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        try:\n",
    "            self._supported_models[self._supported_models.index(m)]\n",
    "        except ValueError as ve:\n",
    "            raise ValueError(f\"Not supported model: {model_name} - {ve.args}\")\n",
    "        model = self._models.get(m)\n",
    "        if not model: # model is not yet initialized\n",
    "            print(f\"{m} is not yet initialized, create a new one!\")\n",
    "            model = models.get_model_builder(m)(pretrained=True).to(device)\n",
    "            model.requires_grad_(False)\n",
    "            model.eval()\n",
    "            self._models[m] = model\n",
    "        else:\n",
    "            print(f\"{m} is already initialized, return directly!\")\n",
    "        return self._models.get(m)\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self._class_labels\n",
    "\n",
    "model_factory = ModelFactory(supported_models)\n",
    "class_labels = model_factory.get_class_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a2b35-3e52-4b06-b524-fe38b9471ac3",
   "metadata": {},
   "source": [
    "# Classify picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e078fc-fc79-4685-b2cc-a9d5306bd915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "def preprocess_image(image_path:str):\n",
    "    preprocessed_image = load_img_to_tensor(image_path)\n",
    "\n",
    "    return preprocessed_image.to(device)\n",
    "\n",
    "def get_logits(model, image_path:str):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        logits = model(preprocessed_image)\n",
    "    return logits\n",
    "\n",
    "# Classify the image\n",
    "def classify_image(model, image_path:str, class_labels:Dict[int, str]):\n",
    "    logits = get_logits(model, image_path)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities.squeeze()).item()\n",
    "    predicted_classname = class_labels[predicted_class]\n",
    "    confidence = probabilities.squeeze()[predicted_class]\n",
    "    return predicted_class, predicted_classname, confidence, probabilities\n",
    "\n",
    "# Classify via tensor\n",
    "def classify_tensor(model, x, class_labels:Dict[int, str]):\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities.squeeze()).item()\n",
    "    predicted_classname = class_labels[predicted_class]\n",
    "    confidence = probabilities.squeeze()[predicted_class]\n",
    "    return predicted_class, predicted_classname, confidence, probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf95e5-0de1-4dd2-b2dc-27664dab1bbc",
   "metadata": {},
   "source": [
    "## Check result of benign examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bb3db-f383-49ff-8a2f-e27691a886ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor_image = load_img_to_tensor(benign_pic).to(device)\n",
    "print(f\"test_tensor_image.shape: {test_tensor_image.shape}\")\n",
    "\n",
    "test_model_name = supported_models[0]\n",
    "test_model = model_factory.get_model(test_model_name)\n",
    "\n",
    "y_hat = F.softmax(test_model(test_tensor_image))\n",
    "\n",
    "test_pred = torch.argmax(y_hat).detach().cpu().item()\n",
    "test_pred_name = class_labels[test_pred]\n",
    "test_confidence = y_hat[0][test_pred]\n",
    "print(f\"Predicted name: {test_pred_name} with index: {test_pred:d}. Confidence: {test_confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a906b-8b5e-42cf-bd50-4fd342ac322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_factory.get_model(model_name)\n",
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "print(f'Predicted class: {predicted_classname} (No: {predicted_class:d}, with confidence: {confidence:.1%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56243ceb-bc0d-4b4c-86fc-85bba1fb7268",
   "metadata": {},
   "source": [
    "# gradient\n",
    "\n",
    "In order to calculate gradient, we'll use cross-entropy loss here:\n",
    "\n",
    "$$\\text{CrossEntropyLoss} = - \\sum{_{i=1}^{N} \\left( y_{i}\\log{(p_{i})}+(1-y_{i})\\log{(1-p_{i})} \\right)}$$\n",
    "\n",
    "Note, the `F.cross_entropy` or `nn.CrossEntropyLoss` is calculated as follows:\n",
    "\n",
    "1. Apply a softmax function to the raw scores to get a probability distribution.\n",
    "2. Compute the negative log-likelihood loss against the given labels.\n",
    "\n",
    "So, the result of the following code is NOT `tensor(0.)` but `tensor(0.9048)` because the `F.cross_entropy` calculates `softmax` first on the input (`pred`) and thus the value of the zeroth index is no longer `1.`:\n",
    "\n",
    "```python\n",
    "loss_fn = F.cross_entropy\n",
    "pred = torch.tensor([[1., 0, 0, 0, 0]])\n",
    "labels = torch.tensor([0])\n",
    "loss_fn(pred, labels)\n",
    "```\n",
    "\n",
    "Comparing with the following code which outputs resulting loss very close to `0`:\n",
    "\n",
    "```python\n",
    "loss_fn = F.cross_entropy\n",
    "pred = torch.tensor([[10., 0, 0, 0, 0]])\n",
    "labels = torch.tensor([0])\n",
    "loss_fn(pred, labels)\n",
    "```\n",
    "\n",
    "Here, because the zeroth-index of `pred` is large enough comparing to others value, after `softmax`, the zeroth-index of the output (the input of the `negative log-likelihood`) is very close to `1.`, the output loss is thus very close the `0.`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2f278-5a7d-472d-924c-4217cdfef05a",
   "metadata": {},
   "source": [
    "## non-target gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac0c37-be0f-491c-874b-1d457a91fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_target_grad(model, pic:str, ground_truth:int, loss_fn=F.cross_entropy):\n",
    "    y = torch.tensor([ground_truth]).to(device)\n",
    "    x = preprocess_image(pic)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    x.requires_grad = True\n",
    "    y_hat = model(x)\n",
    "    loss = -loss_fn(y_hat, y)\n",
    "    print(f\"loss = {loss:.4f}\")\n",
    "    loss.backward()\n",
    "    return x.grad\n",
    "\n",
    "model = model_factory.get_model(model_name)\n",
    "x_grad = non_target_grad(model, pic=benign_pic, ground_truth=predicted_class)\n",
    "\n",
    "print(f\"\\nx_grad.shape = {x_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b46841-e6bd-404f-97ed-77451f743842",
   "metadata": {},
   "source": [
    "## target gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2229c-daa8-4c2c-b1e7-53d014a29025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_grad_by_x(model, x, target:int, loss_fn=F.cross_entropy):\n",
    "    y_target = torch.tensor([target]).to(device)\n",
    "    \n",
    "    x0 = x.detach().to('cpu').clone().to(device) # * imagenet_std + imagenet_mean\n",
    "    x0.requires_grad = True\n",
    "    y_hat = model(x0)\n",
    "    \n",
    "    target_loss = loss_fn(y_hat, y_target)\n",
    "    adv_loss = torch.tensor([0.]).to(device)\n",
    "    other_losses = []\n",
    "    for y_other in class_labels.keys():\n",
    "        if y_other == target:\n",
    "            continue\n",
    "        else:\n",
    "            other_loss = loss_fn(y_hat, torch.tensor([y_other]).to(device))\n",
    "            adv_loss += target_loss / other_loss\n",
    "            other_loss = other_loss.detach().cpu()\n",
    "            other_losses.append(other_loss)\n",
    "\n",
    "    adv_loss.backward()\n",
    "    return x0.grad.cpu(), other_losses, target_loss.cpu(), adv_loss.cpu()\n",
    "\n",
    "def target_grad_by_file(model, pic:str, target:int, loss_fn=F.cross_entropy):\n",
    "    x = preprocess_image(pic)\n",
    "    \n",
    "    x_grad, other_losses, target_loss, adv_loss = target_grad_by_x(model, x, target, loss_fn)\n",
    "    return x_grad, other_losses, target_loss, adv_loss\n",
    "\n",
    "model = model_factory.get_model(model_name)\n",
    "\n",
    "# x_grad, other_losses, target_loss, adv_loss = target_grad_by_file(model, pic=benign_pic, target=target_id)\n",
    "\n",
    "# print(f\"\\nx_grad.shape = {x_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7aa2f-5e22-4aae-b04b-480b797be3c5",
   "metadata": {},
   "source": [
    "# Attacking algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df3582-7c31-4aa5-80ed-3e7206ccf4a5",
   "metadata": {},
   "source": [
    "## fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb270a7d-9a2f-4277-9f13-be8cbd572dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(benign_pic:str, model, predicted_class:int, target_id:int):\n",
    "    x = preprocess_image(benign_pic)\n",
    "    x_grad, other_losses0, target_loss0, adv_loss0 = target_grad_by_file(model, benign_pic, \n",
    "                                                           ground_truth=predicted_class, \n",
    "                                                           target=target_id)\n",
    "    adv_x = x - epsilon * x_grad.sign()\n",
    "    adv_file = tempfile.NamedTemporaryFile().name + \".png\"\n",
    "    store_img_from_tensor(adv_x, adv_file)\n",
    "    print(f\"{'#' * 20}\")\n",
    "    target_grad_by_file(model, adv_file, ground_truth=predicted_class, target=target_id) # See if the loss decreased\n",
    "    return adv_x, x_grad, adv_file\n",
    "\n",
    "# predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "# print(f'Predicted class: {predicted_classname} (No: {predicted_class:d}, with confidence: {confidence:.1%})')\n",
    "\n",
    "# target_id = 508 # computer_keyboard\n",
    "# adv_x, x_grad, adv_file = fgsm(benign_pic, model, predicted_class, target_id)\n",
    "\n",
    "# adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_image(model, adv_file, class_labels)\n",
    "# print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.1%})')\n",
    "\n",
    "# visualize(preprocess_image(benign_pic), adv_x, predicted_class, adv_predicted_class)\n",
    "\n",
    "# os.remove(adv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5ad6d-e244-4206-a2ea-55bb1e36c991",
   "metadata": {},
   "source": [
    "## ifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849529c-4d10-4026-a95c-384fe71ee302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_tensor_image(tensor_image):\n",
    "    scale = 255\n",
    "    tensor_image = tensor_image * scale  # Scale up\n",
    "    tensor_image = torch.round(tensor_image)  # Round\n",
    "    tensor_image = torch.clamp(tensor_image, 0, scale) # each item in tensor_image should be inside [0, 255]\n",
    "    tensor_image = tensor_image / scale  # Scale down\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb9dd4-43a9-48e7-ba49-804999ef28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# size = [3, 256, 256] # (3, 4, 5)\n",
    "# aaa0 = torch.rand(size=size)\n",
    "# for scale in range(200, 10000):\n",
    "#     aaa = clamp_tensor_image(aaa0, scale=scale)\n",
    "#     store_img_from_tensor(aaa, '/tmp/aaa.png')\n",
    "#     aaa1 = load_img_to_tensor('/tmp/aaa.png').squeeze()\n",
    "#     if aaa.allclose(aaa1) and aaa.allclose(aaa0, atol=1e-2):\n",
    "#         print(f\"scale = {scale}\")\n",
    "#         print(\"aaa0:\\n\", aaa0[0][0][:20])\n",
    "#         print(\"aaa:\\n\", aaa[0][0][:20])\n",
    "#         print(\"aaa1:\\n\", aaa1[0][0][:20])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25257ad3-c93a-4b85-9b05-2c831bd9bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifgsm(benign_pic:str, model, target_id:int, num_iterate:int=200, alpha=None):\n",
    "    alpha = alpha if alpha!=None else epsilon / num_iterate\n",
    "    x = preprocess_image(benign_pic).detach().to('cpu')\n",
    "    clip_ratio = 1\n",
    "    clip_arrange = [x + clip_ratio * epsilon, x - clip_ratio * epsilon]\n",
    "    adv_x = x.clone()\n",
    "\n",
    "    other_losses_list = []\n",
    "    target_losses = []\n",
    "    adv_losses = []\n",
    "    display_interval = 10\n",
    "    for i in range(num_iterate):\n",
    "        # adv_file = tempfile.NamedTemporaryFile().name + \".png\"\n",
    "        # store_img_from_tensor(adv_x, adv_file)\n",
    "        x_grad, other_losses, target_loss, adv_loss = target_grad_by_x(\n",
    "            model, x=adv_x, \n",
    "            target=target_id)\n",
    "        \n",
    "        total_other_losses = torch.sum(torch.tensor(other_losses)).item()\n",
    "        \n",
    "        if i % display_interval == 0:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            print(f\"total_other_losses = {total_other_losses:.4f},\\ttarget_loss = {target_loss:.4f},\\tadv_loss = {adv_loss}\")\n",
    "            print(f\"quilt's loss = {other_losses[750]:.4f}\")\n",
    "        \n",
    "        other_losses_list.append(total_other_losses)\n",
    "        target_losses.append(target_loss)\n",
    "        adv_losses.append(adv_loss)\n",
    "\n",
    "        adv_x = adv_x - alpha * x_grad.detach().sign()\n",
    "        adv_x = torch.max(torch.min(adv_x, clip_arrange[0]), clip_arrange[1]) # clip new adv_x back to [x-epsilon, x+epsilon]\n",
    "        adv_x = clamp_tensor_image(adv_x)\n",
    "\n",
    "        # os.remove(adv_file)\n",
    "    \n",
    "    return adv_x, other_losses_list, target_losses, adv_losses\n",
    "\n",
    "# Test\n",
    "\n",
    "model = model_factory.get_model(model_name)\n",
    "\n",
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "print(f\"\\nFor {benign_pic}:\")\n",
    "print(f'Predicted class: {predicted_classname} (No: {predicted_class:d}, with confidence: {confidence:.1%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {probabilities[0][target_id]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4ee77-481a-41b6-9bfa-9cd3d9a21861",
   "metadata": {},
   "source": [
    "# Check result of adversary examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13459835-4468-4d77-8273-e061cb11292f",
   "metadata": {},
   "source": [
    "## Generate adversary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcdd72-1739-42e9-94c4-bf4696d151d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_iterate=50\n",
    "# exclude_list = [predicted_class, 750, 700] # 750: quilt, 700: paper_towel\n",
    "\n",
    "adv_x, other_losses, target_losses, adv_losses = ifgsm(benign_pic, model, target_id, num_iterate=num_iterate, alpha=35*epsilon/num_iterate)\n",
    "adv_file = tempfile.NamedTemporaryFile().name + \".png\"\n",
    "store_img_from_tensor(adv_x, adv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64985f88-1922-4677-8d17-0e3d7761cfcf",
   "metadata": {},
   "source": [
    "## Evaluate using same model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493eaa9f-3760-484f-b216-005ab09dc1be",
   "metadata": {},
   "source": [
    "### via tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8daaa1-6400-4ef0-be1e-e1f6eb21bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_tensor(model, adv_x.to(device), class_labels)\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ff559-5faa-45fb-8366-7738b6f228ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04baeb-766d-4468-9430-409b32df8d5d",
   "metadata": {},
   "source": [
    "### via stored picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9867dc-23c6-4068-8bcd-41d313534bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_img_from_tensor(adv_x, adv_file)\n",
    "adv_x1 = load_img_to_tensor(adv_file)\n",
    "adv_x.allclose(adv_x1, rtol=1e-6, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2323e-b416-4584-8346-f7b3a1f4bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_image(model, adv_file, class_labels)\n",
    "print(f\"\\nFor {adv_file}:\")\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")\n",
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74f14a-8721-4dfa-953b-197b0b79c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copyfile(adv_file, f\"{class_labels[adv_predicted_class]}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712306bd-5ab8-496c-a2c4-194ef271e04d",
   "metadata": {},
   "source": [
    "## Evaluate using defferent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844ae01-5c9f-4102-8590-7c8cad0c3aae",
   "metadata": {},
   "source": [
    "### Get a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad68ef-740d-4a7f-b82f-6fc2722912b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = model_factory.get_model(verify_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c60d0b-0aac-4732-a78f-9f7358159c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_tensor(model, preprocess_image(benign_pic), class_labels)\n",
    "predicted_class, predicted_classname, confidence.item()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e05f4-672f-457b-8477-9c58ee6016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_tensor(verify_model, adv_x.to(device), class_labels)\n",
    "predicted_class, predicted_classname, confidence.item()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c66e02-64ef-49cc-aea1-7abbc2a10eb4",
   "metadata": {},
   "source": [
    "### via tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b227bd-62f2-4294-bdde-85567cfb3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(verify_model, benign_pic, class_labels)\n",
    "# predicted_class, predicted_classname, confidence, probabilities = classify_tensor(verify_model, preprocess_image(benign_pic), class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_tensor(verify_model, adv_x.to(device), class_labels)\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c7209-65af-4535-b92b-1e42e4587677",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93250f-32f6-4307-ba59-65a954b4c6d5",
   "metadata": {},
   "source": [
    "### via stored picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de89b8c-e0a2-4ae7-8c47-3998dba19584",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_img_from_tensor(adv_x, adv_file)\n",
    "adv_x1 = load_img_to_tensor(adv_file)\n",
    "adv_x.allclose(adv_x1, rtol=1e-8, atol=10e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ba9f1-d0b0-4f61-81a4-c002f1f22619",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(verify_model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_image(verify_model, adv_file, class_labels)\n",
    "print(f\"\\nFor {adv_file}:\")\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")\n",
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc769c-3ef7-43a4-8180-d399dfeefb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(adv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lhy Courses",
   "language": "python",
   "name": "lhy_courses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
