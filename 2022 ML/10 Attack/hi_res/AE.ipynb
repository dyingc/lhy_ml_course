{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f43b4-e010-4305-a1da-2a949df9eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from typing import List, Dict\n",
    "import copy\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile, os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "supported_models = ['resnet18', 'resnet50', 'resnet101', 'vgg16_bn', 'vgg19_bn', 'inception_v3']\n",
    "model_name = supported_models[2]\n",
    "verify_model_name = supported_models[4]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if str(device) == 'cpu':\n",
    "    raise RuntimeError(\"cuda is NOT available!!\")\n",
    "\n",
    "benign_pic = './data/cat/Cat04.jpg'\n",
    "benign_pic = './data/duck/Duck02.jpg'\n",
    "benign_pic = './data/mouse/Mouse06.jpg'\n",
    "noise_pic = './data/cat/Cat04.jpg'\n",
    "\n",
    "benign_pic_size = Image.open(benign_pic).size\n",
    "\n",
    "target_id = 508 # computer_keyboard\n",
    "target_id = 99 # goose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8026ab-9a78-4306-9db9-75a8ed3fae7a",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653f2cb-6aa4-4bb3-9dbf-2c673a7e1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean=torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "imagenet_std=torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589593d6-7c04-4765-acc8-e2ca63247fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon = 8 / 255. / imagenet_std\n",
    "epsilon = 8 / 255. * imagenet_std\n",
    "# epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a7d99-5c86-4b78-b7f5-2314f70a43ee",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849529c-4d10-4026-a95c-384fe71ee302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_tensor_image(tensor_image, clip_arrange=None, align_to_image:bool=False):\n",
    "    if clip_arrange!=None and len(clip_arrange)!=2:\n",
    "        raise ValueError(f\"Incorrect len(clip_arrange) ({len(clip_arrange):d})\")\n",
    "\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    if clip_arrange: # No need for `fgsm` as fgsm performs only one time and it can't exceed the limit of [x-epsilon, x+epsilon]\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        tensor_image = torch.max(torch.min(tensor_image, clip_arrange[0]), clip_arrange[1]) # clip new adv_x back to [x-epsilon, x+epsilon]\n",
    "\n",
    "    if align_to_image:\n",
    "        scale = 255\n",
    "        tensor_image = tensor_image * scale  # Scale up\n",
    "        tensor_image = torch.round(tensor_image)  # Round\n",
    "        tensor_image = torch.clamp(tensor_image, 0, scale) # each item in tensor_image should be inside [0, 255]\n",
    "        tensor_image = tensor_image / scale  # Scale down\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe3f25-2e36-4914-9087-c748d3d1d8a5",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e0fa9-fd29-40c3-a211-779db4bcc709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_tensor(noise_pic:str, to_size:List[int]):\n",
    "    to_size = [to_size[-1], to_size[-2]]\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=to_size)])\n",
    "    noise_image = Image.open(noise_pic).convert('RGB')\n",
    "    return transform(noise_image).to(device)\n",
    "noise_tensor0 = get_noise_tensor(noise_pic, benign_pic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a75b63-7c0b-447b-9109-74bdd543ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normed_noise(x, adv):\n",
    "    noise = adv - x # [-1, 1]\n",
    "    # print(f\"min(noise) = {torch.min(noise).item():.4f}, max(noise) = {torch.max(noise).item():.4f}\")\n",
    "    normed_noise = noise / 2 # [-.5, .5]\n",
    "    # print(f\"min(normed_noise) = {torch.min(normed_noise).item():.4f}, max(normed_noise) = {torch.max(normed_noise).item():.4f}\")\n",
    "    normed_noise = normed_noise - torch.min(normed_noise) # [0, ]\n",
    "    # print(f\"min(normed_noise) = {torch.min(normed_noise).item():.4f}, max(normed_noise) = {torch.max(normed_noise).item():.4f}\")\n",
    "    normed_noise = normed_noise / (torch.max(normed_noise) - torch.min(normed_noise)) # [0, 1]\n",
    "    # print(f\"min(normed_noise) = {torch.min(normed_noise).item():.4f}, max(normed_noise) = {torch.max(normed_noise).item():.4f}\")\n",
    "    return normed_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09836601-88b7-4e21-9820-253d58dbeac6",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59caec1-9968-4916-8ec7-8610b4ef9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize(preprocess_image(benign_pic), adv_x, predicted_class, benign_confidence, adv_predicted_class, adv_confidence)\n",
    "def visualize(x, adv, benign_label:int, adv_label:int, benign_confidence:float, adv_confidence:float, height:int=10, width:int=30):\n",
    "    def restore(x):\n",
    "        # x = x * imagenet_std + imagenet_mean\n",
    "        return x\n",
    "\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    x, adv = restore(x.detach().cpu()), restore(adv.detach().cpu())\n",
    "    normed_noise = compute_normed_noise(x, adv)\n",
    "    x, adv, normed_noise = x.numpy().transpose([0, 2, 3, 1]), adv.numpy().transpose([0, 2, 3, 1]), normed_noise.numpy().transpose([0, 2, 3, 1]) # transpose (bs, C, H, W) back to (bs, H, W, C)\n",
    "    plt.figure(figsize=(height, width))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    # predicted_class, predicted_classname, confidence = classify_image(model, benign_pic, class_labels)\n",
    "    plt.title(f\"x: {class_labels[benign_label]} (confidence: {benign_confidence:.1%})\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.squeeze())\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"x_adv: {class_labels[adv_label]} (confidence: {adv_confidence:.1%})\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(adv.squeeze())\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"Noise (x_adv-x), normed to [0, 1]\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(normed_noise.squeeze())\n",
    "    \n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f428e-49b0-4bf5-a763-aa504c56024a",
   "metadata": {},
   "source": [
    "## Store & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106bf6e-140f-43de-9b0d-b140501a9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "def store_img_from_tensor(tensor_image, img_path:str):\n",
    "    if not re.match(r'.*\\.png$', img_path, re.IGNORECASE):\n",
    "        raise TypeError(f\"We have to store image file to png due to the `loss nature of JPEG format`!\")\n",
    "\n",
    "    # tensor_image = tensor_image * imagenet_std + imagenet_mean\n",
    "    # Define the transformation to apply to the tensor\n",
    "    transform = transforms.ToPILImage()\n",
    "    # Apply the transformation to the tensor\n",
    "    pil_image = transform(tensor_image.squeeze())\n",
    "    # Save the PIL image to disk\n",
    "    pil_image.save(img_path) # , quality=100) - not necessary as png, unlike jpeg, will not lose quality\n",
    "    \n",
    "def load_img_to_tensor(img_path:str):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Load the image using PIL\n",
    "    pil_image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Define the transformation to apply to the image\n",
    "    img_loader = transforms.Compose([\n",
    "        transforms.ToTensor(), # Convert the image to a tensor\n",
    "        # transforms.Resize(size=8), # 256),\n",
    "        # transforms.CenterCrop(size=6), #224),\n",
    "        # transforms.Normalize(mean=imagenet_mean, std=imagenet_std) # Normalize the image\n",
    "    ])\n",
    "\n",
    "    # Apply the transformation to the PIL image\n",
    "    tensor_image = img_loader(pil_image)\n",
    "    tensor_image = torch.unsqueeze(tensor_image, 0)  # Add a batch dimension\n",
    "\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57361ea-4168-42a4-96ce-bf8709cdeaf0",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d00f9-0402-416c-b959-791c2ec611b4",
   "metadata": {},
   "source": [
    "Model list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py).\n",
    "\n",
    "Model label list is ImageNet labels which can be found [here](https://files.fast.ai/models/imagenet_class_index.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7873e6-2f99-4ca9-8db5-4d21e2e0d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory():\n",
    "    _instance = None\n",
    "    _supported_models = []\n",
    "    _models = {}\n",
    "    _class_labels = None\n",
    "\n",
    "    def _fill_classlabels(self):\n",
    "        class_file = 'imagenet_class_index.json'\n",
    "        with open(class_file, 'r') as f:\n",
    "            f_contents = f.read()\n",
    "        class_labels = json.loads(f_contents)\n",
    "        self._class_labels = {int(k):v[1] for k, v in class_labels.items()}\n",
    "        \n",
    "    def __new__(self, supported_models, *args, **kwargs):\n",
    "        if not self._instance:\n",
    "            self._instance = super(ModelFactory, self).__new__(self, *args, **kwargs)\n",
    "            self._supported_models = copy.deepcopy(supported_models)\n",
    "            self._models = dict()\n",
    "            self._fill_classlabels(self)\n",
    "        return self._instance\n",
    "            \n",
    "    def get_supported_models(self)->List[str]:\n",
    "        return self.supported_models\n",
    "\n",
    "    def get_model(self, model_name):\n",
    "        m = model_name.lower()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        try:\n",
    "            self._supported_models[self._supported_models.index(m)]\n",
    "        except ValueError as ve:\n",
    "            raise ValueError(f\"Not supported model: {model_name} - {ve.args}\")\n",
    "        model = self._models.get(m)\n",
    "        if not model: # model is not yet initialized\n",
    "            print(f\"{m} is not yet initialized, create a new one!\")\n",
    "            model = models.get_model_builder(m)(pretrained=True).to(device)\n",
    "            model.requires_grad_(False)\n",
    "            model.eval()\n",
    "            self._models[m] = model\n",
    "        else:\n",
    "            print(f\"{m} is already initialized, return directly!\")\n",
    "        return self._models.get(m)\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self._class_labels\n",
    "\n",
    "model_factory = ModelFactory(supported_models)\n",
    "class_labels = model_factory.get_class_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a2b35-3e52-4b06-b524-fe38b9471ac3",
   "metadata": {},
   "source": [
    "# Classify picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e078fc-fc79-4685-b2cc-a9d5306bd915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "def preprocess_image(image_path:str):\n",
    "    preprocessed_image = load_img_to_tensor(image_path)\n",
    "\n",
    "    return preprocessed_image.to(device)\n",
    "\n",
    "def get_logits(model, image_path:str):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        logits = model(preprocessed_image)\n",
    "    return logits\n",
    "\n",
    "# Classify the image\n",
    "def classify_image(model, image_path:str, class_labels:Dict[int, str]):\n",
    "    logits = get_logits(model, image_path)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities.squeeze()).item()\n",
    "    predicted_classname = class_labels[predicted_class]\n",
    "    confidence = probabilities.squeeze()[predicted_class]\n",
    "    return predicted_class, predicted_classname, confidence, probabilities\n",
    "\n",
    "# Classify via tensor\n",
    "def classify_tensor(model, x, class_labels:Dict[int, str]):\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities.squeeze()).item()\n",
    "    predicted_classname = class_labels[predicted_class]\n",
    "    confidence = probabilities.squeeze()[predicted_class]\n",
    "    return predicted_class, predicted_classname, confidence, probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf95e5-0de1-4dd2-b2dc-27664dab1bbc",
   "metadata": {},
   "source": [
    "## Check result of benign examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bb3db-f383-49ff-8a2f-e27691a886ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor_image = load_img_to_tensor(benign_pic).to(device)\n",
    "print(f\"test_tensor_image.shape: {test_tensor_image.shape}\")\n",
    "\n",
    "test_model_name = supported_models[0]\n",
    "test_model = model_factory.get_model(test_model_name)\n",
    "\n",
    "y_hat = F.softmax(test_model(test_tensor_image))\n",
    "\n",
    "test_pred = torch.argmax(y_hat).detach().cpu().item()\n",
    "test_pred_name = class_labels[test_pred]\n",
    "test_confidence = y_hat[0][test_pred]\n",
    "print(f\"Predicted name: {test_pred_name} with index: {test_pred:d}. Confidence: {test_confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a906b-8b5e-42cf-bd50-4fd342ac322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_factory.get_model(model_name)\n",
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "print(f'Predicted class: {predicted_classname} (No: {predicted_class:d}, with confidence: {confidence:.1%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56243ceb-bc0d-4b4c-86fc-85bba1fb7268",
   "metadata": {},
   "source": [
    "# gradient\n",
    "\n",
    "In order to calculate gradient, we'll use cross-entropy loss here:\n",
    "\n",
    "$$\\text{CrossEntropyLoss} = - \\sum{_{i=1}^{N} \\left( y_{i}\\log{(p_{i})}+(1-y_{i})\\log{(1-p_{i})} \\right)}$$\n",
    "\n",
    "Note, the `F.cross_entropy` or `nn.CrossEntropyLoss` is calculated as follows:\n",
    "\n",
    "1. Apply a softmax function to the raw scores to get a probability distribution.\n",
    "2. Compute the negative log-likelihood loss against the given labels.\n",
    "\n",
    "So, the result of the following code is NOT `tensor(0.)` but `tensor(0.9048)` because the `F.cross_entropy` calculates `softmax` first on the input (`pred`) and thus the value of the zeroth index is no longer `1.`:\n",
    "\n",
    "```python\n",
    "loss_fn = F.cross_entropy\n",
    "pred = torch.tensor([[1., 0, 0, 0, 0]])\n",
    "labels = torch.tensor([0])\n",
    "loss_fn(pred, labels)\n",
    "```\n",
    "\n",
    "Comparing with the following code which outputs resulting loss very close to `0`:\n",
    "\n",
    "```python\n",
    "loss_fn = F.cross_entropy\n",
    "pred = torch.tensor([[10., 0, 0, 0, 0]])\n",
    "labels = torch.tensor([0])\n",
    "loss_fn(pred, labels)\n",
    "```\n",
    "\n",
    "Here, because the zeroth-index of `pred` is large enough comparing to others value, after `softmax`, the zeroth-index of the output (the input of the `negative log-likelihood`) is very close to `1.`, the output loss is thus very close the `0.`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2f278-5a7d-472d-924c-4217cdfef05a",
   "metadata": {},
   "source": [
    "## non-target gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac0c37-be0f-491c-874b-1d457a91fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_target_grad(model, pic:str, ground_truth:int, loss_fn=F.cross_entropy):\n",
    "    y = torch.tensor([ground_truth]).to(device)\n",
    "    x = preprocess_image(pic)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    x.requires_grad = True\n",
    "    y_hat = model(x)\n",
    "    loss = -loss_fn(y_hat, y)\n",
    "    print(f\"loss = {loss:.4f}\")\n",
    "    loss.backward()\n",
    "    return x.grad\n",
    "\n",
    "model = model_factory.get_model(model_name)\n",
    "x_grad = non_target_grad(model, pic=benign_pic, ground_truth=predicted_class)\n",
    "\n",
    "print(f\"\\nx_grad.shape = {x_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b46841-e6bd-404f-97ed-77451f743842",
   "metadata": {},
   "source": [
    "## target gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6df88-55c8-4573-bb90-46eee2e731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_grad_by_x0(model, x, target:int, loss_fn=F.cross_entropy):\n",
    "    y_target = torch.tensor([target]).to(device)\n",
    "    \n",
    "    x0 = x.detach().to('cpu').clone().to(device) # * imagenet_std + imagenet_mean\n",
    "    x0.requires_grad = True\n",
    "    y_hat = model(x0)\n",
    "    \n",
    "    target_loss = loss_fn(y_hat, y_target)\n",
    "    adv_loss = torch.tensor([0.]).to(device)\n",
    "    other_losses = []\n",
    "    for y_other in class_labels.keys():\n",
    "        if y_other == target:\n",
    "            continue\n",
    "        else:\n",
    "            other_loss = loss_fn(y_hat, torch.tensor([y_other]).to(device))\n",
    "            adv_loss += target_loss / other_loss\n",
    "            other_loss = other_loss.detach().cpu()\n",
    "            other_losses.append(other_loss)\n",
    "\n",
    "    adv_loss.backward()\n",
    "    return x0.grad.cpu(), other_losses, target_loss.cpu(), adv_loss.cpu()\n",
    "\n",
    "# The function computes adversarial examples for a given model, initial input, target class, expected noise and learning rate.\n",
    "def target_grad_by_x1(model, x, target:int, expected_noise, alpha:float, loss_fn=F.cross_entropy):\n",
    "    # Preprocessing the benign picture\n",
    "    benign_x = preprocess_image(benign_pic)\n",
    "\n",
    "    # Detaching 'x' from the current graph and making it requires gradient computation\n",
    "    x0 = x.detach().clone()\n",
    "    x0.requires_grad = True\n",
    "\n",
    "    # Forward pass through the model\n",
    "    y_hat = model(x0)\n",
    "\n",
    "    # Creating the target tensor\n",
    "    y_target = torch.tensor([target], device=device)\n",
    "    # Calculating the target loss\n",
    "    target_loss = loss_fn(y_hat, y_target)\n",
    "    \n",
    "    # Initializing the adversarial loss\n",
    "    adv_loss = torch.tensor([0.], device=device)\n",
    "    other_losses = []\n",
    "    \n",
    "    # Calculating the adversarial loss for other class labels\n",
    "    for other in class_labels.keys():\n",
    "        # Skip if the class label is the same as the target\n",
    "        if other != target:\n",
    "            # Create a tensor for the other class\n",
    "            y_other = torch.tensor([other]).to(device)\n",
    "            # Compute the loss with the other class\n",
    "            other_loss = loss_fn(y_hat, y_other)\n",
    "            # Add the ratio of target loss to other loss to the adversarial loss\n",
    "            adv_loss += target_loss / other_loss\n",
    "            other_losses.append(other_loss.cpu())\n",
    "\n",
    "    # Averaging the adversarial loss\n",
    "    adv_loss = adv_loss / len(other_losses) if other_losses else 0\n",
    "    \n",
    "    # Backpropagation for adversarial loss\n",
    "    adv_loss.backward(retain_graph=True)\n",
    "    \n",
    "    # Applying the gradient to x0\n",
    "    adv_x = x0 - alpha * x0.grad.sign()\n",
    "\n",
    "    # Compute the noise between the benign image and the adversarial example\n",
    "    noise_tensor = compute_normed_noise(benign_x, adv_x)\n",
    "    # Compute the noise loss\n",
    "    noise_loss = F.mse_loss(expected_noise, noise_tensor)\n",
    "    # Compute the total loss as the square root of the sum of the squares of the adversarial and noise losses\n",
    "    total_loss = torch.sqrt(adv_loss ** 2 + noise_loss ** 2)\n",
    "\n",
    "    # Clearing the gradients of all optimized tensors\n",
    "    model.zero_grad()\n",
    "    # Backpropagation for the total loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    return x0.grad, other_losses, target_loss.detach().cpu(), noise_loss.detach().cpu(), total_loss.detach().cpu()\n",
    "\n",
    "\n",
    "def target_grad_by_file(model, pic:str, target:int, loss_fn=F.cross_entropy):\n",
    "    x = preprocess_image(pic)\n",
    "    \n",
    "    x_grad, other_losses, target_loss, adv_loss = target_grad_by_x0(model, x, target, loss_fn)\n",
    "    return x_grad, other_losses, target_loss, adv_loss\n",
    "\n",
    "model = model_factory.get_model(model_name)\n",
    "x = preprocess_image(benign_pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24609902-06c2-47c7-bc22-d12f71b49db1",
   "metadata": {},
   "source": [
    "# Merge cat and dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51675d34-5599-416f-81a2-cb0bf69bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic01 = \"./data/cat/Cat01.jpg\"\n",
    "pic02 = \"./data/dog/Dog01.jpg\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7aa2f-5e22-4aae-b04b-480b797be3c5",
   "metadata": {},
   "source": [
    "# Attacking algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df3582-7c31-4aa5-80ed-3e7206ccf4a5",
   "metadata": {},
   "source": [
    "## fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb270a7d-9a2f-4277-9f13-be8cbd572dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(benign_pic:str, model, target_id:int):\n",
    "    x = preprocess_image(benign_pic)\n",
    "    x_grad, other_losses0, target_loss0, adv_loss0 = target_grad_by_file(model, \n",
    "                                                                         benign_pic, \n",
    "                                                                         target=target_id)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    adv_x = x.detach().cpu() - epsilon * x_grad.detach().sign()\n",
    "    adv_x = clamp_tensor_image(adv_x)\n",
    "    return adv_x, x_grad, other_losses0, target_loss0, adv_loss0\n",
    "\n",
    "adv_x, x_grad, other_losses0, target_loss0, adv_loss0 = fgsm(benign_pic, model, target_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5ad6d-e244-4206-a2ea-55bb1e36c991",
   "metadata": {},
   "source": [
    "## ifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25257ad3-c93a-4b85-9b05-2c831bd9bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifgsm(benign_pic:str, model, target_id:int, num_iterate:int=200, alpha=None):\n",
    "    alpha = alpha if alpha!=None else epsilon.to(device) / num_iterate\n",
    "    x = preprocess_image(benign_pic).detach()\n",
    "    clip_ratio = 1\n",
    "    clip_arrange = [x + clip_ratio * epsilon.to(device), x - clip_ratio * epsilon.to(device)]\n",
    "    adv_x = x.clone()\n",
    "\n",
    "    other_losses_list = []\n",
    "    target_losses = []\n",
    "    adv_losses = []\n",
    "    display_interval = 25\n",
    "    for i in range(num_iterate):\n",
    "        # x_grad, other_losses, target_loss, adv_loss = target_grad_by_x0(model, x=adv_x, target=target_id)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        x_grad, other_losses, target_loss, noise_loss, adv_loss = target_grad_by_x1(model, x=adv_x, target=target_id, expected_noise=noise_tensor0.unsqueeze(0), alpha=alpha)\n",
    "        \n",
    "        avg_other_losses = torch.mean(torch.tensor(other_losses)).item()\n",
    "        \n",
    "        if i % display_interval == 0:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            print(f\"avg_other_losses = {avg_other_losses:.8f},\\ttarget_loss = {target_loss:.8f},\\tnoise_loss = {noise_loss:.8f},\\tadv_loss = {adv_loss.item():.8f}\")\n",
    "        \n",
    "        other_losses_list.append(avg_other_losses)\n",
    "        target_losses.append(target_loss)\n",
    "        adv_losses.append(adv_loss)\n",
    "        # print(f\"Before change: adv_x - x = {torch.sum(adv_x-x).item():.8f}\")\n",
    "        adv_x = adv_x - alpha * x_grad.detach().sign()\n",
    "        # print(f\"After change: adv_x - x = {torch.sum(adv_x-x).item():.8f}\")\n",
    "        adv_x = clamp_tensor_image(adv_x, clip_arrange, align_to_image=False)\n",
    "        # adv_x1 = clamp_tensor_image(adv_x1, clip_arrange, align_to_image=False)\n",
    "        # adv_x = adv_x1\n",
    "        # print(f\"After clamp_tensor_image: adv_x - x = {torch.sum(adv_x-x).item():.8f}\")\n",
    "\n",
    "        # os.remove(adv_file)\n",
    "    adv_x = clamp_tensor_image(adv_x, clip_arrange, align_to_image=True)\n",
    "    # adv_x1 = clamp_tensor_image(adv_x1, clip_arrange, align_to_image=True)\n",
    "\n",
    "\n",
    "    \n",
    "    return adv_x, other_losses_list, target_losses, adv_losses\n",
    "\n",
    "# Test\n",
    "\n",
    "model = model_factory.get_model(model_name)\n",
    "\n",
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "print(f\"\\nFor {benign_pic}:\")\n",
    "print(f'Predicted class: {predicted_classname} (No: {predicted_class:d}, with confidence: {confidence:.1%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {probabilities[0][target_id]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4ee77-481a-41b6-9bfa-9cd3d9a21861",
   "metadata": {},
   "source": [
    "# Check result of adversary examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13459835-4468-4d77-8273-e061cb11292f",
   "metadata": {},
   "source": [
    "## Generate adversary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcdd72-1739-42e9-94c4-bf4696d151d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_iterate=45\n",
    "\n",
    "adv_x, other_losses, target_losses, adv_losses = ifgsm(benign_pic, model, target_id, num_iterate=num_iterate, alpha=35*epsilon.to(device)/num_iterate)\n",
    "# adv_x, x_grad, other_losses, target_loss, adv_loss = fgsm(benign_pic, model, target_id)\n",
    "adv_file = tempfile.NamedTemporaryFile().name + \".png\"\n",
    "store_img_from_tensor(adv_x, adv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba8435-4b57-4d6a-a51d-e4990eed8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_tensor(model, adv_x.to(device), class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e54793-857b-449f-a06d-297ae41fb974",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence, height=20, width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64985f88-1922-4677-8d17-0e3d7761cfcf",
   "metadata": {},
   "source": [
    "## Evaluate using same model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493eaa9f-3760-484f-b216-005ab09dc1be",
   "metadata": {},
   "source": [
    "### via tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8daaa1-6400-4ef0-be1e-e1f6eb21bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_tensor(model, adv_x.to(device), class_labels)\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ff559-5faa-45fb-8366-7738b6f228ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence, height=20, width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04baeb-766d-4468-9430-409b32df8d5d",
   "metadata": {},
   "source": [
    "### via stored picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2323e-b416-4584-8346-f7b3a1f4bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_image(model, adv_file, class_labels)\n",
    "print(f\"\\nFor {adv_file}:\")\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")\n",
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74f14a-8721-4dfa-953b-197b0b79c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copyfile(adv_file, f\"{class_labels[adv_predicted_class]}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712306bd-5ab8-496c-a2c4-194ef271e04d",
   "metadata": {},
   "source": [
    "## Evaluate using defferent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844ae01-5c9f-4102-8590-7c8cad0c3aae",
   "metadata": {},
   "source": [
    "### Get a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad68ef-740d-4a7f-b82f-6fc2722912b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = model_factory.get_model(verify_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c60d0b-0aac-4732-a78f-9f7358159c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_tensor(model, preprocess_image(benign_pic), class_labels)\n",
    "predicted_class, predicted_classname, confidence.item()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e05f4-672f-457b-8477-9c58ee6016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_tensor(verify_model, adv_x.to(device), class_labels)\n",
    "predicted_class, predicted_classname, confidence.item()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c66e02-64ef-49cc-aea1-7abbc2a10eb4",
   "metadata": {},
   "source": [
    "### via tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b227bd-62f2-4294-bdde-85567cfb3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(verify_model, benign_pic, class_labels)\n",
    "# predicted_class, predicted_classname, confidence, probabilities = classify_tensor(verify_model, preprocess_image(benign_pic), class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_tensor(verify_model, adv_x.to(device), class_labels)\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c7209-65af-4535-b92b-1e42e4587677",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93250f-32f6-4307-ba59-65a954b4c6d5",
   "metadata": {},
   "source": [
    "### via stored picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de89b8c-e0a2-4ae7-8c47-3998dba19584",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_img_from_tensor(adv_x, adv_file)\n",
    "adv_x1 = load_img_to_tensor(adv_file).to(device)\n",
    "adv_x.allclose(adv_x1, rtol=1e-8, atol=10e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ba9f1-d0b0-4f61-81a4-c002f1f22619",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_classname, confidence, probabilities = classify_image(verify_model, benign_pic, class_labels)\n",
    "adv_predicted_class, adv_predicted_classname, adv_confidence, adv_probabilities = classify_image(verify_model, adv_file, class_labels)\n",
    "print(f\"\\nFor {adv_file}:\")\n",
    "print(f'Predicted class: {adv_predicted_classname} (No: {adv_predicted_class:d}, with confidence: {adv_confidence:.4%}).')\n",
    "print(f\"The target classname is: {class_labels[target_id]} (No. {target_id}) with confidence: {adv_probabilities[0][target_id]:.4%}\")\n",
    "visualize(x=preprocess_image(benign_pic), adv=adv_x, benign_label=predicted_class, adv_label=adv_predicted_class, benign_confidence=confidence, adv_confidence=adv_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc769c-3ef7-43a4-8180-d399dfeefb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(adv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lhy Courses",
   "language": "python",
   "name": "lhy_courses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
